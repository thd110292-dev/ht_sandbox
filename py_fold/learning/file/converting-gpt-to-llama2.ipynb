{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cfa9090-2aa6-455b-99b5-5bd792c1a185",
   "metadata": {},
   "source": [
    "# Convert GPT model to LLAMA2.\n",
    "## Reason: Because LLaMA2 has several \"modern\" optimizations that make it more efficient and accurate for large scale language tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050df82e-be8e-4fc8-88f3-d44069153bae",
   "metadata": {},
   "source": [
    "## Replacing the layerNorm with RMSNORM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2366bbcb-3984-476a-8ddd-c47a91dfac1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbformat in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (5.10.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from nbformat) (2.21.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from nbformat) (4.26.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from nbformat) (5.9.1)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from nbformat) (5.14.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat) (0.30.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat) (4.15.0)\n",
      "Requirement already satisfied: blobfile in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.8 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from blobfile) (3.23.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from blobfile) (2.6.1)\n",
      "Requirement already satisfied: lxml>=4.9 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from blobfile) (6.0.2)\n",
      "Requirement already satisfied: filelock>=3.0 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from blobfile) (3.20.0)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from huggingface_hub) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from huggingface_hub) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from huggingface_hub) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from huggingface_hub) (6.0.3)\n",
      "Requirement already satisfied: shellingham in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from huggingface_hub) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from huggingface_hub) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub) (0.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from typer-slim->huggingface_hub) (8.3.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tiktoken) (2026.1.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n",
      "Requirement already satisfied: torch in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from torch) (2026.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\justa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp310-cp310-win_amd64.whl.metadata (10 kB)\n",
      "Downloading sentencepiece-0.2.1-cp310-cp310-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 7.2 MB/s  0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nbformat\n",
    "!pip install blobfile\n",
    "!pip install huggingface_hub\n",
    "!pip install matplotlib\n",
    "!pip install tiktoken\n",
    "!pip install torch\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56e49375-fb03-4d2d-9eeb-af9475d665f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load LLama2 component\n",
    "\n",
    "# this will replace the LayerNorm with RMSNORM Layer , this improve computational efficiency\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-5):\n",
    "        super().__init__() # tell python to run the initialization code of the parent class, in this case since we create class based on nn (pytorch) \n",
    "        self.eps = eps # define epsilon \n",
    "        self.emb_dim = emb_dim # define embedding dimension size \n",
    "        self.weight = nn.Parameter(torch.ones(emb_dim)).float()\n",
    "\n",
    "    def forward(self, x): # this is use to Replace LayerNorm with RMSNorm layer (in order to transform LLAMA 2 to gpt) imporve computational efficiency\n",
    "        means = x.pow(2).mean(dim=-1, keepdim=True) # set up the mean. where mean(dim=-1, keepin = True) tell python calculate the average of those squared value across the last dimension\n",
    "        x_normed = x * torch.rsqrt(means + self.eps) # this will normalize the input\n",
    "        return (x_normed * self.weight).to(dtype=x.dtype) # apply the learnable weight here \n",
    "\n",
    "''' run it / check '''\n",
    "torch.manual_seed(123)\n",
    "\n",
    "example_batch = torch.randn(2, 3, 4)\n",
    "\n",
    "rms_norm = RMSNorm(emb_dim=example_batch.shape[-1])\n",
    "rmsnorm_pytorch = torch.nn.RMSNorm(example_batch.shape[-1], eps=1e-5) # pytorch implementation, replaycing the standard layer normalization with the rsmnorm \n",
    "\n",
    "assert torch.allclose(rms_norm(example_batch), rmsnorm_pytorch(example_batch)) # this is the validation step. This out come is sielnt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3984b26f-61bf-4418-8ab7-86423c969972",
   "metadata": {},
   "source": [
    "## Step 2: Replace GELU with SiLU activation\n",
    "### Note: LLAMA use the SiLU activation (Sigmoid Linear Unit) and GPT use GELU (Gaussian Error Linear Unit). Both are smooth alternative, SiLU often is more computationally efficient and become the preferred choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f609960-1026-45c8-9193-d9e6af1e5646",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiLU(nn.Module):# define a class \n",
    "    def __init__(self):\n",
    "        super(SiLU, self).__init__() # this line here connect my customer class to pytorch framework.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)# take everynumber in the input and pressed down between 0 and 1\n",
    "\n",
    "# Validation \n",
    "silu = SiLU()\n",
    "\n",
    "assert torch.allclose(silu(example_batch), torch.nn.functional.silu(example_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e937083-f908-4d45-9773-0b40be4f3fbd",
   "metadata": {},
   "source": [
    "## Update the feed-forward Network (FFN) \n",
    "### NOTE: b/c LLAMA use more sophisticated structure (SwiGLU) instead of the standard two-layer MLP found in GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91bdfe9e-39aa-4448-8dda-55b19ef8a925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape validation passed: torch.Size([2, 3, 4])\n",
      "Mathematical logic validation passed!\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg): # cfg is the configuration \n",
    "        super().__init__()\n",
    "        # in LLAMA the feed forward network require 3 layer instead of two layer\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False) # take the data and expand it into a higher dimension\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False) # Gate layer. Run in parallel to fc1, help the model decide which information is important\n",
    "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False) # output projection layer. Combine the results from prior layers and shrinks it down to the original emb_dim (demnsion)\n",
    "        self.silu = SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        x = self.silu(x_fc1) * x_fc2\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Validate \n",
    "# 1. Setup a dummy configuration matching your notebook's style\n",
    "test_cfg = {\n",
    "    \"emb_dim\": 4,\n",
    "    \"hidden_dim\": 8,\n",
    "    \"dtype\": torch.float32\n",
    "}\n",
    "\n",
    "# 2. Initialize your FeedForward layer\n",
    "model_ffn = FeedForward(test_cfg)\n",
    "\n",
    "# 3. Create dummy input data (Batch size=2, Seq length=3, Emb dim=4)\n",
    "test_input = torch.randn(2, 3, 4)\n",
    "\n",
    "# 4. Pass the input through your layer\n",
    "output = model_ffn(test_input)\n",
    "\n",
    "# 5. Validation Assertions\n",
    "# Check if the output shape matches the input shape (Standard for FFNs)\n",
    "assert output.shape == test_input.shape\n",
    "print(f\"Shape validation passed: {output.shape}\")\n",
    "\n",
    "# Manual verification of the SwiGLU logic:\n",
    "# output = fc3( silu(fc1(x)) * fc2(x) )\n",
    "with torch.no_grad():\n",
    "    x_fc1 = model_ffn.fc1(test_input)\n",
    "    x_fc2 = model_ffn.fc2(test_input)\n",
    "    manual_swiglu = model_ffn.silu(x_fc1) * x_fc2\n",
    "    expected_output = model_ffn.fc3(manual_swiglu)\n",
    "\n",
    "assert torch.allclose(output, expected_output)\n",
    "print(\"Mathematical logic validation passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f92335-3d38-4199-aa32-6cdddd2a8b0d",
   "metadata": {},
   "source": [
    "## Implement RoPE (rotary positional embedding). Core architectual differences between GPT and Llama2 \n",
    "### NOTE: bc gpt use absolute Positional embeddings. By using this we have \n",
    "### 1. relative position awareness\n",
    "### 2. Infinite Length Extrapolation \n",
    "### 3. Mathematicall efficiency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab16cbbc-2924-40d4-8725-0d2fd3137d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_rope_params(head_dim, theta_base=10_000, context_length=4096):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\" # RoPE work by rotating pairs of number. CANT change these number it will always be 2 \n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim)) # calculate the frequencies (rotational speeds), can adjust to make it faster. Increase the value will extend the context window of a model.\n",
    "\n",
    "    # Generate position indices\n",
    "    positions = torch.arange(context_length) # this create a list of number from 0 up to 4095, represent each possible position in a sentence.\n",
    "\n",
    "    # Compute the angles\n",
    "    angles = positions.unsqueeze(1) * inv_freq.unsqueeze(0)  # Shape: (context_length, head_dim // 2) # it multiple each position by every rotation speed to create a unique 'angle' for each dimension at every position.\n",
    "\n",
    "    # Expand angles to match the head_dim\n",
    "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim) # duplicate the angles so the first half and the second half of my word can be rotate insync.\n",
    "\n",
    "    # Precompute sine and cosine. Converting raw angles into cosine and sine values. \n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "def compute_rope(x, cos, sin):\n",
    "    # x is the input data\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape # extract the dimension of the input data X.\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\" # setup for RoPE\n",
    "\n",
    "    # Split x into first half and second half\n",
    "    x1 = x[..., : head_dim // 2]  # First half\n",
    "    x2 = x[..., head_dim // 2 :]  # Second half\n",
    "\n",
    "    # Adjust sin and cos shapes. Since precomputed table might be very large. \n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Apply the rotary transformation\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    return x_rotated.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53f9ade5-9a3f-4c5b-be44-2c1f15b37d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([2, 4, 5, 16])\n",
      "Rotated shape:  torch.Size([2, 4, 5, 16])\n",
      "\n",
      "First query vector (before RoPE):\n",
      "tensor([ 0.3374, -0.1778, -0.3035, -0.5880])\n",
      "First query vector (after RoPE):\n",
      "tensor([ 0.3374, -0.1778, -0.3035, -0.5880])\n",
      "\n",
      "Original magnitude: 3.1241\n",
      "Rotated magnitude:  3.1241\n"
     ]
    }
   ],
   "source": [
    "''' VALIDATION / TEST '''\n",
    "# Input\n",
    "batch_size = 2\n",
    "context_len = 5\n",
    "num_heads = 4\n",
    "head_dim = 16\n",
    "\n",
    "# Instantiate RoPE parameters\n",
    "cos, sin = precompute_rope_params(head_dim=head_dim, context_length=context_len)\n",
    "\n",
    "# Dummy query and key tensors\n",
    "torch.manual_seed(123)\n",
    "queries = torch.randn(batch_size, num_heads, context_len, head_dim)\n",
    "keys = torch.randn(batch_size, num_heads, context_len, head_dim)\n",
    "\n",
    "# Apply rotary position embeddings\n",
    "queries_rot = compute_rope(queries, cos, sin)\n",
    "keys_rot = compute_rope(keys, cos, sin)\n",
    "\n",
    "# Check that the shapes remain the same\n",
    "print(f\"Original shape: {queries.shape}\")\n",
    "print(f\"Rotated shape:  {queries_rot.shape}\")\n",
    "\n",
    "# Verify that the values have actually changed\n",
    "print(f\"\\nFirst query vector (before RoPE):\\n{queries[0, 0, 0, :4]}\")\n",
    "print(f\"First query vector (after RoPE):\\n{queries_rot[0, 0, 0, :4]}\")\n",
    "\n",
    "# Optional: Verify that the rotation preserved the vector length (magnitude)\n",
    "# Rotation shouldn't change the 'energy' of the vector\n",
    "print(f\"\\nOriginal magnitude: {torch.norm(queries[0, 0, 0]):.4f}\")\n",
    "print(f\"Rotated magnitude:  {torch.norm(queries_rot[0, 0, 0]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f07f84d-b110-43de-9048-bbea83dce11a",
   "metadata": {},
   "source": [
    "## Add RoPE for MultiHeadAttention module\n",
    "### B/c in a standard GPT model, positional information is added to the word embedding at the very beginning of the network. LlaMA2 positions are injected inside every attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bbe71c1-a099-4209-bb99-f4b12b241682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Chapter 3\n",
    "#####################################\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, num_heads, dtype=None):  # ,dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\" # initial the RoPE\n",
    "\n",
    "        self.d_out = d_out # output dimension of attion layer \n",
    "        self.num_heads = num_heads #number of attention heads \n",
    "        self.head_dim = d_out // num_heads  # calculate the size of each individual attention head. Reduce the projection dim to match desired output dim\n",
    "\n",
    "        ################################### NEW ###################################\n",
    "        # Set bias=False and dtype=dtype for all linear layers below\n",
    "        ###########################################################################\n",
    "\n",
    "        ''' these are the layers projectmy input into query, key and value vector'''\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype) \n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)  # Linear layer to combine head outputs\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)) # create a causal mask - ensure that token \"t\" can only attend to tokens \\leq t, not future token\n",
    "\n",
    "        ################################### NEW ###################################\n",
    "        cos, sin = precompute_rope_params(head_dim=self.head_dim, context_length=context_length) # pre-calculated the  the rotation value for the entire possible context length\n",
    "        '''save these rotation values as buffer so they move automatically to GPU with the GPU''' \n",
    "        self.register_buffer(\"cos\", cos)\n",
    "        self.register_buffer(\"sin\", sin)\n",
    "        ###########################################################################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape  # extract the batch size \n",
    "\n",
    "        '''  linear layer that create three specialized version of my data ''' \n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out) # what each word has to 'offer' to other words.\n",
    "        queries = self.W_query(x) # what each word is looking for in the sentence\n",
    "        values = self.W_value(x) # the actual 'meaning' or content that gets passed forward\n",
    "\n",
    "        \n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim) \n",
    "        # rearrange the data so the head are in the correct position for parallel math operations. \n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Apply LlaMA2 positional logic (i.e. RoPE) #\n",
    "        keys = compute_rope(keys, self.cos, self.sin)\n",
    "        queries = compute_rope(queries, self.cos, self.sin) # LLaMA specific upgrade. it rotate the queries and keys based on the position that we preset (cosine and sine) \n",
    "        ###########################################################################\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head # compare every words 'question' (Q) against every other words 'label' )K)\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf) # apply causal mask. ensure that model only look at past words and cannot 'cheat' by looking at future word\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1) # converts raw scores into percentages that add up to 100%\n",
    "        # attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) # multiples the percentage by the \"meaning\" (V) to create a final weighted representation\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out) # glues the multiple head back together \n",
    "        context_vec = self.out_proj(context_vec)  # optional projection. This is a final layer that \"clean\" up and refine combined result before passing it to the next part of the model\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d28c9fda-20cd-4119-bf59-e039e7250e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (W_key): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (W_value): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (out_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# example of using mutiHeadAttention\n",
    "# Settings\n",
    "batch_size = 1\n",
    "context_len = 100\n",
    "max_context_len = 4096\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "\n",
    "\n",
    "example_batch = torch.randn((batch_size, context_len, embed_dim))\n",
    "\n",
    "mha = MultiHeadAttention(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=max_context_len,\n",
    "    num_heads=num_heads\n",
    ")\n",
    "\n",
    "mha(example_batch)\n",
    "print(mha)\n",
    "# del mha  # delete to free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5447c18c-1cb8-42c5-b1f0-f4435b0207d8",
   "metadata": {},
   "source": [
    "## Update the transformerBlock Module \n",
    "### this is the final step to glue together all the individual Llama2 components that you have build i.e. (RMSNorm, MultiHeadAttention, and FeedForward Network) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46f3fbca-444a-470d-b33d-21c8b2b4149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dtype=cfg[\"dtype\"]  # NEW\n",
    "            # dropout=cfg[\"drop_rate\"],\n",
    "            # qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "\n",
    "        ################################### REplace layerNorm with RMSNorm ###################################\n",
    "        # self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        # self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm1 = RMSNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = RMSNorm(cfg[\"emb_dim\"])\n",
    "        ###########################################################################\n",
    "\n",
    "        # self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        # x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        # x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1108ea6f-2446-45ac-b293-62a3a78e6f4b",
   "metadata": {},
   "source": [
    "## Update the Model Class \n",
    "### Not since transformerBlock is a repeated block within the main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cf3415e-362d-4e8e-b26c-8bb027e3c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GPTModel(nn.Module):\n",
    "class Llama2Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"]) # create token embedding layer. Turn input word IDs (integers) into dense vector\n",
    "        # self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        # self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]) # craete n_layers transformer blocks and stack them one after another. The outputs of one block becomes the input of the next block \n",
    "\n",
    "        ################################### Replace LayerNorm with RMSNorm  ###################################\n",
    "        # self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        # batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        # pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds  # + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        # x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403f0e8c-f84f-4b72-86b6-367ecde35036",
   "metadata": {},
   "source": [
    "# Final step - TEST run with GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7582ef2b-a24b-4831-94b4-554ba325d34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 6,738,415,616\n"
     ]
    }
   ],
   "source": [
    "# state the config\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "GPT_CONFIG_1558M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 1600,         # Embedding dimension\n",
    "    \"n_heads\": 25,           # Number of attention heads\n",
    "    \"n_layers\": 48,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "LLAMA2_CONFIG_7B = {\n",
    "    \"vocab_size\": 32000,     # Vocabulary size\n",
    "    \"context_length\": 4096,  # Context length\n",
    "    \"emb_dim\": 4096,         # Embedding dimension\n",
    "    \"n_heads\": 32,           # Number of attention heads\n",
    "    \"n_layers\": 32,          # Number of layers\n",
    "    \"hidden_dim\": 11008,     # NEW: Size of the intermediate dimension in FeedForward\n",
    "    \"dtype\": torch.bfloat16  # NEW: Lower-precision dtype to reduce memory usage\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = Llama2Model(LLAMA2_CONFIG_7B)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c77c4c8-fbb6-4a4e-ac47-4c2720964684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (PyTorch default): 52.33 GB\n",
      "bfloat16: 26.17 GB\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Calculate the the memory require to run the model Llama2Model using  the following\n",
    "'''\n",
    "def model_memory_size(model, input_dtype=torch.float32):\n",
    "    total_params = 0\n",
    "    total_grads = 0\n",
    "    for param in model.parameters():\n",
    "        # Calculate total number of elements per parameter\n",
    "        param_size = param.numel()\n",
    "        total_params += param_size\n",
    "        # Check if gradients are stored for this parameter\n",
    "        if param.requires_grad:\n",
    "            total_grads += param_size\n",
    "\n",
    "    # Calculate buffer size (non-parameters that require memory)\n",
    "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
    "\n",
    "    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
    "    # We assume parameters and gradients are stored in the same type as input dtype\n",
    "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
    "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
    "\n",
    "    # Convert bytes to gigabytes\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "\n",
    "    return total_memory_gb\n",
    "\n",
    "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
    "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c8385-530d-4b57-8fd3-128ee3e87668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-pytorch_env]",
   "language": "python",
   "name": "conda-env-anaconda3-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
